#!/bin/bash

LITELLM_MASTER_KEY=litellm-647e8dd5-7197-4639-899f-463408b3fb44
LITELLM_SALT_KEY=litellm-3efab3d2-e033-46fe-976f-f2f05115ef98
ENABLE_NETWORK_MONITOR=true
LOG_LEVEL=DEBUG

export LLAMA_CPP_BIN="$HOME/apps/llama.cpp/build/bin/"
export PATH="$LLAMA_CPP_BIN:$PATH"

export MODEL_LOCATIONS="$HOME/Boxxed/@dev/models"

reset_claude_environment_variables(){
    unset ANTHROPIC_AUTH_TOKEN
    unset ANTHROPIC_BASE_URL
    unset ANTHROPIC_MODEL
    unset ANTHROPIC_SMALL_FAST_MODEL
    unset CLAUDE_CODE_MAX_OUTPUT_TOKENS
}

llamacpp_glm_start(){
    which_file="$1"
    if [[ -z $which_file ]]; then
        # Too big: 
        #
        # which_file="GLM-4.7-Flash-Q4_K_M.gguf"
        # which_file="GLM-4.7-Flash-Q4_K_M.gguf"
        # which_file="GLM-4.7-Flash-Q4_K_S.gguf"
        # which_file="GLM-4.7-Flash-Q3_K_M.gguf"
        #
        # Smaller models with good precision
        # which_file="GLM-4.7-Flash-UD-Q4_K_XL.gguf"
        # which_file="GLM-4.7-Flash-UD-Q3_K_XL.gguf"
        # which_file="GLM-4.7-Flash-UD-Q2_K_XL.gguf"
        which_file="GLM-4.7-Flash-UD-Q3_K_XL.gguf"
        # which_file="GLM-4.7-Flash-UD-Q4_K_XL.gguf"
    fi

    # For general use-case:  --temp 1.0 --top-p 0.95
    # For tool-calling:  --temp 0.7 --top-p 1.0
    # Default ctx: --ctx-size 16384 \
    # Other arguments you can use:
    #
    # --ctx-size 202752 \
    # --n-gpu-layers 99 \
    #
    # context size =   202752
    # output tokens =  128000
    # output tokens =  64000
    # recomended was = 16384

    llama-server \
    --model "$MODEL_LOCATIONS/unsloth/GLM-4.7-Flash-GGUF/${which_file}" \
    --alias "unsloth/GLM-4.7-Flash" \
    --flash-attn auto \
    --threads -1 \
    --seed 3407 \
    --temp 0.6 \
    --top-p 1.0 \
    --min-p 0.01 \
    --fit on \
    --sleep-idle-seconds 600 \
    --repeat-penalty 1.0 \
    --ctx-size 128000 \
    --port 8001 \
    --jinja
}


claude_llamacpp_start() {
    reset_claude_environment_variables .

    export ANTHROPIC_AUTH_TOKEN="llamacpp"
    export ANTHROPIC_BASE_URL="http://localhost:8001"
    export CLAUDE_CODE_MAX_OUTPUT_TOKENS="64000"

    model="$1"
    if [[ -z $model ]]; then 
        model="unsloth/GLM-4.7-Flash"
    fi

    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        claude --model "$model"
    fi
}

claude_llamacpp_ralph() {
    reset_claude_environment_variables .

    export ANTHROPIC_AUTH_TOKEN="llamacpp"
    export ANTHROPIC_BASE_URL="http://localhost:8001"
    export CLAUDE_CODE_MAX_OUTPUT_TOKENS="64000"
    # context size =   202752
    # output tokens =  64000
    # recomended was = 16384

    prompt_type="$1"
    shift
    prompt_data="$*"

    model="unsloth/GLM-4.7-Flash"

    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        if [[ $prompt_type == "file" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            cat $prompt_data | claude --dangerously-skip-permissions --model=$model
        elif [[ $prompt_type == "message" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            echo "$prompt_data" | claude --dangerously-skip-permissions --model=$model
        else
            echo "Unknown prompt_type: '$prompt_type', only supports: ('message', 'file')."
        fi
    fi
}


sglang_unsloth_glm_start(){
    which_file="$1"
    if [[ -z $which_file ]]; then
        which_file="GLM-4.7-Flash-UD-Q4_K_XL.gguf"
    fi

    # --model-path "$MODEL_LOCATIONS/unsloth/GLM-4.7-Flash/${which_file}" \

    python3 -m sglang.launch_server \
        --model-path "zai-org/GLM-4.7-Flash"
        --tp-size 4 \
        --tool-call-parser glm47  \
        --reasoning-parser glm45 \
        --speculative-algorithm EAGLE \
        --speculative-num-steps 3 \
        --speculative-eagle-topk 1 \
        --speculative-num-draft-tokens 4 \
        --mem-fraction-static 0.8 \
        --served-model-name glm-4.7-flash \
        --host 0.0.0.0 \
        --port 8000
}


glm_start(){
    python3 -m sglang.launch_server \
        --model-path "$MODEL_LOCATIONS/zai-org/GLM-4.7-Flash" \
        --tp-size 4 \
        --tool-call-parser glm47  \
        --reasoning-parser glm45 \
        --speculative-algorithm EAGLE \
        --speculative-num-steps 3 \
        --speculative-eagle-topk 1 \
        --speculative-num-draft-tokens 4 \
        --mem-fraction-static 0.8 \
        --served-model-name glm-4.7-flash \
        --host 0.0.0.0 \
        --port 8000
}

litellm_start(){
    if has_command "pip"; then
        if no_command "litellm"; then
            echo "Installing litellm"
            pip install 'litellm[proxy]'
        fi
    else
        echo "Unable to install litellm with pip, please setup"
    fi

    if no_command "litellm"; then
        echo "No litellm for me to run with"
    else
        litellm --config $DOTFILES/config/litellm/copilot-config.yaml
    fi
}


ollama_start(){
    if has_command "brew"; then
        if no_command "ollama"; then
            echo "Installing ollama"
            sudo pacman -S --noconfirm ollama-cuda cuda nvtop 
        fi
    else
        echo "Unable to install ollama with brew, please setup"
    fi

    if no_command "ollama"; then
        echo "No ollama for me to run with"
    else
        # # First, create the array
        # mapfile -t GPU_UUIDS < <(nvidia-smi -L | grep -oP '(?<=UUID: )GPU-[a-f0-9-]+')
        #
        # # Method 2: Join with comma
        # IFS=',' GPU_UUIDS_COMMA="${GPU_UUIDS[*]}"
        # echo "Identified GPU UUDs: $GPU_UUIDS_COMMA"
        #
        # export CUDA_VISIBLE_DEVICES="$GPU_UUIDS_COMMA"
        # echo "Setting CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"

        ollama serve
    fi
}


claude_ollama_start() {
    reset_claude_environment_variables .

    export ANTHROPIC_AUTH_TOKEN="ollama"
    export ANTHROPIC_BASE_URL="http://localhost:11434"

    model="$1"
    if [[ -z $model ]]; then 
        model="gpt-oss:20b"
        # model="qwen3-coder:30b"
    fi

    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        claude --model "$model"
    fi
}

opencode_start(){
    reset_claude_environment_variables

    if has_command "curl"; then
        if no_command "opencode"; then
            echo "Installing opencode"
            curl -fsSL https://opencode.ai/install | bash
        fi
    else
        echo "Unable to install opencode with curl, please setup"
    fi

    model="$1"
    if [[ -z $model ]]; then 
        # model="gpt-oss:20b"
        model="qwen3-coder:30b"
    fi

    if no_command "opencode"; then
        echo "No opencode for me to run with"
    else
        opencode --model=$model
    fi
}

opencode_ralph(){
    reset_claude_environment_variables

    if has_command "curl"; then
        if no_command "opencode"; then
            echo "Installing opencode"
            curl -fsSL https://opencode.ai/install | bash
        fi
    else
        echo "Unable to install opencode with curl, please setup"
    fi

    model="$1"
    shift
    prompt_type="$1"
    shift
    prompt_data="$*"

    if no_command "opencode"; then
        echo "No opencode for me to run with"
    else
        if [[ $prompt_type == "file" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            cat $prompt_data | opencode run --model "$model" 
        elif [[ $prompt_type == "message" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            echo "$prompt_data" | opencode run --model "$model"
        else
            echo "Unknown prompt_type: '$prompt_type', only supports: ('message', 'file')."
        fi
    fi
}

test_llama_claude() {
    curl --location 'http://localhost:11434/v1/chat/completions' \
        --header "Authorization: Bearer ollama" \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "gpt-oss:20b",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
}

claude_ralph() {
    reset_claude_environment_variables

    export ANTHROPIC_AUTH_TOKEN="$LITELLM_MASTER_KEY"
    export ANTHROPIC_BASE_URL="http://localhost:4000"
    export ANTHROPIC_MODEL="claude-sonnet-4.5"
    export ANTHROPIC_SMALL_FAST_MODEL="gpt-4"
    export CLAUDE_CODE_MAX_OUTPUT_TOKENS="64000"

    prompt_type="$1"
    shift
    prompt_data="$*"

    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        if [[ $prompt_type == "file" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            cat $prompt_data | claude --dangerously-skip-permissions
        elif [[ $prompt_type == "message" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            echo "$prompt_data" | claude --dangerously-skip-permissions
        else
            echo "Unknown prompt_type: '$prompt_type', only supports: ('message', 'file')."
        fi
    fi
}

claude_start() {
    export ANTHROPIC_AUTH_TOKEN="$LITELLM_MASTER_KEY"
    export ANTHROPIC_BASE_URL="http://localhost:4000"
    export ANTHROPIC_MODEL="claude-sonnet-4.5"
    export ANTHROPIC_SMALL_FAST_MODEL="gpt-4"
    export CLAUDE_CODE_MAX_OUTPUT_TOKENS="64000"
    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        claude
    fi
}

test_gemini_flash() {
    curl --location 'http://0.0.0.0:4000/chat/completions' \
        --header "Authorization: Bearer $LITELLM_MASTER_KEY" \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "gemini-3-flash",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
}

test_claude_opus() {
    curl --location 'http://0.0.0.0:4000/chat/completions' \
        --header "Authorization: Bearer $LITELLM_MASTER_KEY" \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "claude-opus-4",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
}

test_claude_haiku() {
    curl --location 'http://0.0.0.0:4000/chat/completions' \
        --header "Authorization: Bearer $LITELLM_MASTER_KEY" \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "claude-haiku-4.5",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
}

test_claude_sonnet() {
    curl --location 'http://0.0.0.0:4000/chat/completions' \
        --header "Authorization: Bearer $LITELLM_MASTER_KEY" \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "claude-sonnet-4",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
    
}

test_claude_sonnet45() {
    curl --location 'http://0.0.0.0:4000/chat/completions' \
        --header "Authorization: Bearer $LITELLM_MASTER_KEY" \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "claude-sonnet-4.5",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
}

test_gpt4() {
    curl --location 'http://0.0.0.0:4000/chat/completions' \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "gpt-4",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
}

test_models() {
    curl --location 'http://0.0.0.0:4000/v1/models' \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' 
}
