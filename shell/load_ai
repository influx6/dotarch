#!/bin/bash

LITELLM_MASTER_KEY=litellm-647e8dd5-7197-4639-899f-463408b3fb44
LITELLM_SALT_KEY=litellm-3efab3d2-e033-46fe-976f-f2f05115ef98
ENABLE_NETWORK_MONITOR=true
LOG_LEVEL=DEBUG

BASE_LLM_CONTEXT=132000

export LLAMA_CPP_BIN="$HOME/apps/llama.cpp/build/bin/"
export PATH="$LLAMA_CPP_BIN:$PATH"

export MODEL_LOCATIONS="$HOME/Boxxed/@dev/models"

reset_claude_environment_variables(){
    unset ANTHROPIC_AUTH_TOKEN
    unset ANTHROPIC_BASE_URL
    unset ANTHROPIC_MODEL
    unset ANTHROPIC_SMALL_FAST_MODEL
    unset CLAUDE_CODE_MAX_OUTPUT_TOKENS
}

hf_download(){
    repo="$1"
    shift

    file_name="$1"
    if [[ ! -z $file_name ]]; then 
        shift
        hf download --local-dir="$HOME/Boxxed/@dev/models/$repo" $repo $file_name $*
    else
        hf download --local-dir="$HOME/Boxxed/@dev/models/$repo" $repo $*
    fi

}

######################################
#  OpenCode 
######################################

opencode_start(){
    reset_claude_environment_variables

    if has_command "curl"; then
        if no_command "opencode"; then
            echo "Installing opencode"
            curl -fsSL https://opencode.ai/install | bash
        fi
    else
        echo "Unable to install opencode with curl, please setup"
    fi

    model="$1"
    if [[ -z $model ]]; then 
        model="llamacpp/unsloth/Qwen3-Coder-30B"
    fi

    if no_command "opencode"; then
        echo "No opencode for me to run with"
    else
        opencode --model=$model
    fi
}

opencode_ralph(){
    reset_claude_environment_variables

    if has_command "curl"; then
        if no_command "opencode"; then
            echo "Installing opencode"
            curl -fsSL https://opencode.ai/install | bash
        fi
    else
        echo "Unable to install opencode with curl, please setup"
    fi

    model="$1"
    if [[ -z $model ]]; then 
        model="llamacpp/unsloth/Qwen3-Coder-30B"
    fi

    shift
    prompt_type="$1"
    shift
    prompt_data="$*"

    if no_command "opencode"; then
        echo "No opencode for me to run with"
    else
        if [[ $prompt_type == "file" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            prompt_data_content=$(cat $prompt_data)
            echo "Executing: opencode --model \"$model\" --prompt \"#(cat $prompt_data)\""
            opencode --model "$model" "Start work on: @$prompt_data"
        elif [[ $prompt_type == "message" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            echo "Executing: opencode --model \"$model\" --prompt \"$prompt_data\""
            opencode --model "$model" --prompt "$prompt_data"
        else
            echo "Unknown prompt_type: '$prompt_type', only supports: ('message', 'file')."
        fi
    fi
}


opencode_glm47_flash(){
    opencode_start "llamacpp/unsloth/GLM-4.7-Flash" $*
}

opencode_qwen3_coder(){
    opencode_start "llamacpp/unsloth/Qwen3-Coder-30B" $*
}

opencode_qwen3_4b(){
    opencode_start "llamacpp/unsloth/Qwen3-4B" $*
}

opencode_qwen3_30b(){
    opencode_start "llamacpp/unsloth/Qwen3-30B" $*
}

opencode_ralph_glm47_flash(){
    opencode_ralph "llamacpp/unsloth/GLM-4.7-Flash" $*
}

opencode_ralph_qwen3_coder_30b(){
    opencode_ralph "llamacpp/unsloth/Qwen3-Coder-30B" $*
}

opencode_ralph_qwen3_4b(){
    opencode_ralph "llamacpp/unsloth/Qwen3-4B" $*
}

opencode_ralph_qwen3_30b(){
    opencode_ralph "llamacpp/unsloth/Qwen3-30B" $*
}

######################################
#  ClaudeCode 
######################################

claude_qwen3_start() {
    reset_claude_environment_variables .

    export ANTHROPIC_AUTH_TOKEN="llamacpp"
    export ANTHROPIC_BASE_URL="http://localhost:8001"
    export CLAUDE_CODE_MAX_OUTPUT_TOKENS="64000"

    model="$1"
    if [[ -z $model ]]; then 
        model="Qwen3-4B"
    fi

    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        rm -rf ~/.claude/settings.json
        claude --model "$model"
    fi
}

claude_qwen3_ralph() {
    reset_claude_environment_variables .

    export ANTHROPIC_AUTH_TOKEN="llamacpp"
    export ANTHROPIC_BASE_URL="http://localhost:8001"
    export CLAUDE_CODE_MAX_OUTPUT_TOKENS="64000"
    # context size =   202752
    # output tokens =  64000
    # recomended was = 16384

    prompt_type="$1"
    shift
    prompt_data="$*"

    model="Qwen3-4B"

    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        rm -rf ~/.claude/settings.json
        if [[ $prompt_type == "file" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            cat $prompt_data | claude --dangerously-skip-permissions --model=$model
        elif [[ $prompt_type == "message" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            echo "$prompt_data" | claude --dangerously-skip-permissions --model=$model
        else
            echo "Unknown prompt_type: '$prompt_type', only supports: ('message', 'file')."
        fi
    fi
}

llamacpp_no_offload_start(){
    model_dir="$1"
    model_file="$2"
    model_alias="$3"

    which_temp="$4"
    if [[ -z $which_temp ]]; then
        which_temp="0.5"
    fi


    # For general use-case:  --temp 1.0 --top-p 0.95
    # For tool-calling:  --temp 0.7 --top-p 1.0
    # Default ctx: --ctx-size 16384 \
    # Other arguments you can use:
    #
    # --ctx-size 202752 \
    # --n-gpu-layers 99 \
    #
    # context size =   202752
    # output tokens =  128000
    # output tokens =  64000
    # recomended was = 16384
    #
    #
    # Offloading
    #
    #If you have more VRAM, you can try offloading more MoE layers, or offloading whole layers themselves.
    #
    # Normally, -ot ".ffn_.*_exps.=CPU" offloads all MoE layers to the CPU! This effectively allows you 
    # to fit all non MoE layers on 1 GPU, improving generation speeds. You can customize the regex expression 
    # to fit more layers if you have more GPU capacity.
    #
    # If you have a bit more GPU memory, try -ot ".ffn_(up|down)_exps.=CPU" This offloads up and down projection MoE layers.
    #
    # Try -ot ".ffn_(up)_exps.=CPU" if you have even more GPU memory. This offloads only up projection MoE layers.
    #
    # You can also customize the regex, for example -ot "\.(6|7|8|9|[0-9][0-9]|[0-9][0-9][0-9])\.ffn_(gate|up|down)_exps.=CPU" 
    # means to offload gate, up and down MoE layers but only from the 6th layer onwards.
    #
    echo "Starting llama-server with: \"$MODEL_LOCATIONS/$model_dir/${model_file}\""

    llama-server \
    --model "$MODEL_LOCATIONS/$model_dir/${model_file}" \
    --alias "$model_alias" \
    --fit on \
    -ngl 99 \
    --threads -1 \
    --seed 3407 \
    --temp $which_temp \
    --top-p 0.8 \
    --top-k 20 \
    --min-p 0.01 \
    --repeat-penalty 1.0 \
    --sleep-idle-seconds 600 \
    --ctx-size $BASE_LLM_CONTEXT \
    --port 8001 \
    --jinja
}

llamacpp_moe_offload_start(){
    model_dir="$1"
    model_file="$2"
    model_alias="$3"

    which_temp="$4"
    if [[ -z $which_temp ]]; then
        which_temp="0.5"
    fi


    # For general use-case:  --temp 1.0 --top-p 0.95
    # For tool-calling:  --temp 0.7 --top-p 1.0
    # Default ctx: --ctx-size 16384 \
    # Other arguments you can use:
    #
    # --ctx-size 202752 \
    # --n-gpu-layers 99 \
    #
    # context size =   202752
    # output tokens =  128000
    # output tokens =  64000
    # recomended was = 16384
    #
    #
    # Offloading
    #
    #If you have more VRAM, you can try offloading more MoE layers, or offloading whole layers themselves.
    #
    # Normally, -ot ".ffn_.*_exps.=CPU" offloads all MoE layers to the CPU! This effectively allows you 
    # to fit all non MoE layers on 1 GPU, improving generation speeds. You can customize the regex expression 
    # to fit more layers if you have more GPU capacity.
    #
    # If you have a bit more GPU memory, try -ot ".ffn_(up|down)_exps.=CPU" This offloads up and down projection MoE layers.
    #
    # Try -ot ".ffn_(up)_exps.=CPU" if you have even more GPU memory. This offloads only up projection MoE layers.
    #
    # You can also customize the regex, for example -ot "\.(6|7|8|9|[0-9][0-9]|[0-9][0-9][0-9])\.ffn_(gate|up|down)_exps.=CPU" 
    # means to offload gate, up and down MoE layers but only from the 6th layer onwards.
    #
    echo "Starting llama-server with: \"$MODEL_LOCATIONS/$model_dir/${model_file}\""

    llama-server \
    --model "$MODEL_LOCATIONS/$model_dir/${model_file}" \
    --alias "$model_alias" \
    --fit on \
    -ngl 99 \
    --threads -1 \
    --seed 3407 \
    --temp $which_temp \
    --top-p 0.8 \
    --top-k 20 \
    --min-p 0.01 \
    --repeat-penalty 1.0 \
    -ot ".ffn_.*_exps.=CPU" \
    --sleep-idle-seconds 600 \
    --ctx-size $BASE_LLM_CONTEXT \
    --port 8001 \
    --jinja
}

claude_qwen3_30b_coder_ralph() {
    reset_claude_environment_variables .

    export ANTHROPIC_AUTH_TOKEN="llamacpp"
    export ANTHROPIC_BASE_URL="http://localhost:8001"
    export CLAUDE_CODE_MAX_OUTPUT_TOKENS="64000"
    # context size =   202752
    # output tokens =  64000
    # recomended was = 16384

    prompt_type="$1"
    shift
    prompt_data="$*"

    model="Qwen3-Coder-30B"

    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        rm -rf ~/.claude/settings.json
        if [[ $prompt_type == "file" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            cat $prompt_data | claude --dangerously-skip-permissions --model=$model
        elif [[ $prompt_type == "message" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            echo "$prompt_data" | claude --dangerously-skip-permissions --model=$model
        else
            echo "Unknown prompt_type: '$prompt_type', only supports: ('message', 'file')."
        fi
    fi
}


llamacpp_qwen3_coder_start(){
    which_temp="$1"
    if [[ -z $which_temp ]]; then
        which_temp="0.25"
    fi

    # model_file="Qwen3-Coder-30B-A3B-Instruct-1M-UD-Q3_K_XL.gguf"
    # model_file="Qwen3-Coder-30B-A3B-Instruct-1M-Q3_K_M.gguf"
    # model_file="Qwen3-Coder-30B-A3B-Instruct-1M-Q3_K_S.gguf"
    model_file="Qwen3-Coder-30B-A3B-Instruct-1M-IQ4_XS.gguf"
    model_dir="unsloth/Qwen3-Coder-30B-A3B-Instruct-1M-GGUF"
    model_alias="Qwen3-Coder-30B"

    llamacpp_moe_offload_start $model_dir $model_file $model_alias $which_temp
}

llamacpp_glm_thinking_start(){
    model_file="GLM-4.7-Flash-UD-Q4_K_XL.gguf"

    which_temp="$1"
    if [[ -z $which_temp ]]; then
        which_temp="0.25"
    fi
    
    llamacpp_no_offload_start "unsloth/GLM-4.7-Flash-GGUF" $model_file "unsloth/GLM-4.7-Flash" $which_temp
}


llamacpp_qwen3_thinking_start(){
    # model_file="Qwen3-4B-Instruct-2507-UD-Q8_K_XL.gguf"
    model_file="Qwen3-4B-Thinking-2507-F16.gguf"

    which_temp="$1"
    if [[ -z $which_temp ]]; then
        which_temp="0.25"
    fi

    model_dir="unsloth/Qwen3-4B-Thinking-2507-GGUF"
    model_alias="Qwen3-4B"

    llamacpp_no_offload_start $model_dir $model_file $model_alias $which_temp
}

######################################
#   Qwen3 Coder 30B
######################################

claude_qwen3_30b_coder_start() {
    reset_claude_environment_variables .

    export ANTHROPIC_AUTH_TOKEN="llamacpp"
    export ANTHROPIC_BASE_URL="http://localhost:8001"
    export CLAUDE_CODE_MAX_OUTPUT_TOKENS="64000"

    model="$1"
    if [[ -z $model ]]; then 
        model="Qwen3-Coder-30B"
    fi

    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        rm -rf ~/.claude/settings.json
        claude --model "$model"
    fi
}

claude_qwen3_30b_coder_ralph() {
    reset_claude_environment_variables .

    export ANTHROPIC_AUTH_TOKEN="llamacpp"
    export ANTHROPIC_BASE_URL="http://localhost:8001"
    export CLAUDE_CODE_MAX_OUTPUT_TOKENS="64000"
    # context size =   202752
    # output tokens =  64000
    # recomended was = 16384

    prompt_type="$1"
    shift
    prompt_data="$*"

    model="Qwen3-Coder-30B"

    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        rm -rf ~/.claude/settings.json
        if [[ $prompt_type == "file" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            cat $prompt_data | claude --dangerously-skip-permissions --model=$model
        elif [[ $prompt_type == "message" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            echo "$prompt_data" | claude --dangerously-skip-permissions --model=$model
        else
            echo "Unknown prompt_type: '$prompt_type', only supports: ('message', 'file')."
        fi
    fi
}

llamacpp_qwen3_coder_instruct_start(){
    which_temp="$1"
    if [[ -z $which_temp ]]; then
        which_temp="0.25"
    fi

    model_file="Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL.gguf"
    model_dir="unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"
    model_alias="Qwen3-Coder-30B"

    llamacpp_moe_offload_start $model_dir $model_file $model_alias $which_temp
}

llamacpp_qwen3_coder_3kxl_instruct_start(){
    which_temp="$1"
    if [[ -z $which_temp ]]; then
        which_temp="0.25"
    fi

    model_file="Qwen3-Coder-30B-A3B-Instruct-UD-Q3_K_XL.gguf"
    model_dir="unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"
    model_alias="Qwen3-Coder-30B"

    llamacpp_moe_offload_start $model_dir $model_file $model_alias $which_temp
}

llamacpp_qwen3_coder_4ixs_instruct_start(){
    which_temp="$1"
    if [[ -z $which_temp ]]; then
        which_temp="0.25"
    fi

    model_file="Qwen3-Coder-30B-A3B-Instruct-IQ4_XS.gguf"
    model_dir="unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"
    model_alias="Qwen3-Coder-30B"

    llamacpp_moe_offload_start $model_dir $model_file $model_alias $which_temp
}

llamacpp_qwen3_coder_3km_instruct_start(){
    which_temp="$1"
    if [[ -z $which_temp ]]; then
        which_temp="0.25"
    fi

    model_file="Qwen3-Coder-30B-A3B-Instruct-Q3_K_M.gguf"
    model_dir="unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"
    model_alias="Qwen3-Coder-30B"

    llamacpp_moe_offload_start $model_dir $model_file $model_alias $which_temp
}

llamacpp_qwen3_coder_4km_instruct_start(){
    which_temp="$1"
    if [[ -z $which_temp ]]; then
        which_temp="0.25"
    fi

    model_file="Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf"
    model_dir="unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"
    model_alias="Qwen3-Coder-30B"

    llamacpp_moe_offload_start $model_dir $model_file $model_alias $which_temp
}

llamacpp_qwen3_coder_41_instruct_start(){
    which_temp="$1"
    if [[ -z $which_temp ]]; then
        which_temp="0.25"
    fi

    model_file="Qwen3-Coder-30B-A3B-Instruct-Q4_1.gguf"
    model_dir="unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"
    model_alias="Qwen3-Coder-30B"

    llamacpp_moe_offload_start $model_dir $model_file $model_alias $which_temp
}

######################################
#   Qwen3-30B
######################################

claude_qwen3_30b_start() {
    reset_claude_environment_variables .

    export ANTHROPIC_AUTH_TOKEN="llamacpp"
    export ANTHROPIC_BASE_URL="http://localhost:8001"
    export CLAUDE_CODE_MAX_OUTPUT_TOKENS="64000"

    model="$1"
    if [[ -z $model ]]; then 
        model="Qwen3-30B"
    fi

    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        rm -rf ~/.claude/settings.json
        claude --model "$model"
    fi
}

claude_qwen3_30b_ralph() {
    reset_claude_environment_variables .

    export ANTHROPIC_AUTH_TOKEN="llamacpp"
    export ANTHROPIC_BASE_URL="http://localhost:8001"
    export CLAUDE_CODE_MAX_OUTPUT_TOKENS="64000"
    # context size =   202752
    # output tokens =  64000
    # recomended was = 16384

    prompt_type="$1"
    shift
    prompt_data="$*"

    model="Qwen3-30B"

    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        rm -rf ~/.claude/settings.json
        if [[ $prompt_type == "file" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            cat $prompt_data | claude --dangerously-skip-permissions --model=$model
        elif [[ $prompt_type == "message" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            echo "$prompt_data" | claude --dangerously-skip-permissions --model=$model
        else
            echo "Unknown prompt_type: '$prompt_type', only supports: ('message', 'file')."
        fi
    fi
}

llamacpp_qwen3_30b_instruct_start(){
    which_temp="$1"
    if [[ -z $which_temp ]]; then
        which_temp="0.25"
    fi
    model_file="Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf"
    model_dir="unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF"
    model_alias="Qwen3-30B"

    llamacpp_no_offload_start $model_dir $model_file $model_alias $which_temp
}

llamacpp_qwen3_30b_4ks_instruct_start(){
    which_temp="$1"
    if [[ -z $which_temp ]]; then
        which_temp="0.25"
    fi

    model_file="Qwen3-30B-A3B-Instruct-2507-Q4_K_S.gguf"
    model_dir="unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF"
    model_alias="Qwen3-30B"

    llamacpp_no_offload_start $model_dir $model_file $model_alias $which_temp
}

llamacpp_qwen3_30b_4xs_instruct_start(){
    which_temp="$1"
    if [[ -z $which_temp ]]; then
        which_temp="0.25"
    fi

    model_file="Qwen3-30B-A3B-Instruct-2507-IQ4_XS.gguf"
    model_dir="unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF"
    model_alias="Qwen3-30B"

    llamacpp_no_offload_start $model_dir $model_file $model_alias $which_temp
}

llamacpp_qwen3_30b_40_instruct_start(){
    which_temp="$1"
    if [[ -z $which_temp ]]; then
        which_temp="0.25"
    fi
    model_file="Qwen3-30B-A3B-Instruct-2507-Q4_0.gguf"
    model_dir="unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF"
    model_alias="Qwen3-30B"

    llamacpp_no_offload_start $model_dir $model_file $model_alias $which_temp
}

llamacpp_qwen3_30b_ud4xl_instruct_start(){
    which_temp="$1"
    if [[ -z $which_temp ]]; then
        which_temp="0.25"
    fi
    model_file="Qwen3-30B-A3B-Instruct-2507-UD-Q4_K_XL.gguf"
    model_dir="unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF"
    model_alias="Qwen3-30B"

    llamacpp_no_offload_start $model_dir $model_file $model_alias $which_temp
}

llamacpp_qwen3_30b_3ks_instruct_start(){
    which_temp="$1"
    if [[ -z $which_temp ]]; then
        which_temp="0.25"
    fi
    model_dir="unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF"
    model_file="Qwen3-30B-A3B-Instruct-2507-Q3_K_S.gguf"
    model_alias="Qwen3-30B"

    llamacpp_no_offload_start $model_dir $model_file $model_alias $which_temp
}

llamacpp_qwen3_30b_3km_instruct_start(){
    which_temp="$1"
    if [[ -z $which_temp ]]; then
        which_temp="0.25"
    fi
    model_file="Qwen3-30B-A3B-Instruct-2507-Q3_K_M.gguf"
    model_dir="unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF"
    model_alias="Qwen3-30B"

    llamacpp_no_offload_start $model_dir $model_file $model_alias $which_temp
}

llamacpp_qwen3_30b_3kxl_instruct_start(){
    which_temp="$1"
    if [[ -z $which_temp ]]; then
        which_temp="0.25"
    fi
    model_file="Qwen3-30B-A3B-Instruct-2507-UD-Q3_K_XL.gguf"
    model_dir="unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF"
    model_alias="Qwen3-30B"

    llamacpp_no_offload_start $model_dir $model_file $model_alias $which_temp
}


claude_qwen3_coder_start() {
    reset_claude_environment_variables .

    export ANTHROPIC_AUTH_TOKEN="llamacpp"
    export ANTHROPIC_BASE_URL="http://localhost:8001"
    export CLAUDE_CODE_MAX_OUTPUT_TOKENS="64000"

    model="$1"
    if [[ -z $model ]]; then 
        model="Qwen3-Coder-30B"
    fi

    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        rm -rf ~/.claude/settings.json
        claude --model "$model"
    fi
}

claude_qwen3_coder_ralph() {
    reset_claude_environment_variables .

    export ANTHROPIC_AUTH_TOKEN="llamacpp"
    export ANTHROPIC_BASE_URL="http://localhost:8001"
    export CLAUDE_CODE_MAX_OUTPUT_TOKENS="64000"
    # context size =   202752
    # output tokens =  64000
    # recomended was = 16384

    prompt_type="$1"
    shift
    prompt_data="$*"

    model="Qwen3-Coder-30B"

    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        rm -rf ~/.claude/settings.json
        if [[ $prompt_type == "file" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            cat $prompt_data | claude --dangerously-skip-permissions --model=$model
        elif [[ $prompt_type == "message" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            echo "$prompt_data" | claude --dangerously-skip-permissions --model=$model
        else
            echo "Unknown prompt_type: '$prompt_type', only supports: ('message', 'file')."
        fi
    fi
}


llamacpp_glm_start(){
    model_file="$1"
    if [[ -z $model_file ]]; then
        # Too big: 
        #
        # model_file="GLM-4.7-Flash-Q4_K_M.gguf"
        # model_file="GLM-4.7-Flash-Q4_K_M.gguf"
        # model_file="GLM-4.7-Flash-Q4_K_S.gguf"
        # model_file="GLM-4.7-Flash-Q3_K_M.gguf"
        #
        # Smaller models with good precision
        # model_file="GLM-4.7-Flash-UD-Q4_K_XL.gguf"
        # model_file="GLM-4.7-Flash-UD-Q3_K_XL.gguf"
        # model_file="GLM-4.7-Flash-UD-Q2_K_XL.gguf"
        # model_file="GLM-4.7-Flash-UD-Q3_K_XL.gguf"
        model_file="GLM-4.7-Flash-UD-Q4_K_XL.gguf"
    fi

    which_temp="$2"
    if [[ -z $which_temp ]]; then
        which_temp="0.7"
    fi

    # For general use-case:  --temp 1.0 --top-p 0.95
    # For tool-calling:  --temp 0.7 --top-p 1.0
    # Default ctx: --ctx-size 16384 \
    # Other arguments you can use:
    #
    # --ctx-size 202752 \
    # --n-gpu-layers 99 \
    #
    # context size =   202752
    # output tokens =  128000
    # output tokens =  64000
    # recomended was = 16384
    #
    #
    # Thinking can be turned off with either:
    #
    # --chat-template-kwargs '{"thinking": false}'
    #
    # OR 
    #
    # --reasoning-budget 0
    #
    #
    # Recommendations:
    # Yes, for coding tasks with GLM-4.7-Flash, you should lower the temperature below 0.5, with recommended settings 
    # often falling between 0.2 and 0.3 for maximum accuracy and code generation precision. 
    # While some general-purpose configurations suggest 0.7–1.0 for speed, reducing the temperature to 0.2 
    # makes the model's output more deterministic and reliable, preventing unnecessary creative deviations in code structure. 
    # Recommended Settings for GLM-4.7-Flash Coding 
    # Temperature: 0.2 (0.1–0.3 range)
    # Top-P: 0.9 or 0.95 (Maintains high probability tokens while allowing slight variation for better syntax)
    # Top-K: 50 (often recommended to limit the token pool for better accuracy)
    # Use Case: If the code is for production, stick to 0.2. If it is for quick experimentation/prompting, 0.6–0.7 may be acceptable, but 0.2–0.3 reduces bugs. 
    # Key Considerations
    # Low Temp = Less Errors: For programming, you want the model to be deterministic and follow best practices. 
    # A temperature below 0.5 prevents "hallucinating" unnecessary loops or using deprecated syntax.
    # Reasoning Mode Behavior: If you are using GLM-4.7 Flash with "thinking mode" enabled, it may take longer to process, 
    # but low temperatures help keep the reasoning focused.
    # What if it's too rigid? If the code feels too repetitive or simple, slightly increase the temperature to 0.4 or 0.5, 
    # but rarely higher than that for coding. 
    # Disclaimer: Based on research from January 2026, including local deployment guides using tools like llama.cpp 
    # and API documentation for GLM-4.7-Flash.

    llama-server \
    --model "$MODEL_LOCATIONS/unsloth/GLM-4.7-Flash-GGUF/${model_file}" \
    --alias "unsloth/GLM-4.7-Flash" \
    --reasoning-budget 0 \
    --flash-attn auto \
    -ngl 99 \
    --threads -1 \
    --seed 3407 \
    --temp $which_temp \
    --top-p 0.95 \
    --min-p 0.01 \
    --fit on \
    --sleep-idle-seconds 600 \
    --repeat-penalty 1.05 \
    --ctx-size $BASE_LLM_CONTEXT \
    --port 8001 \
    --jinja
}

llamacpp_glm_creative_coding_start(){
    model_file="$1"
    if [[ -z $model_file ]]; then
        # Too big: 
        #
        # model_file="GLM-4.7-Flash-Q4_K_M.gguf"
        # model_file="GLM-4.7-Flash-Q4_K_M.gguf"
        # model_file="GLM-4.7-Flash-Q4_K_S.gguf"
        # model_file="GLM-4.7-Flash-Q3_K_M.gguf"
        #
        # Smaller models with good precision
        # model_file="GLM-4.7-Flash-UD-Q4_K_XL.gguf"
        # model_file="GLM-4.7-Flash-UD-Q3_K_XL.gguf"
        # model_file="GLM-4.7-Flash-UD-Q2_K_XL.gguf"
        # model_file="GLM-4.7-Flash-UD-Q3_K_XL.gguf"
        model_file="GLM-4.7-Flash-UD-Q4_K_XL.gguf"
    fi

    which_temp="$2"
    if [[ -z $model_file ]]; then
        which_temp="0.45"
    fi

    llamacpp_glm_start "$model_file" "$which_temp"
}

llamacpp_glm_coding_start(){
    model_file="$1"
    if [[ -z $model_file ]]; then
        model_file="GLM-4.7-Flash-UD-Q4_K_XL.gguf"
    fi

    which_temp="$2"
    if [[ -z $model_file ]]; then
        which_temp="0.25"
    fi

    llamacpp_glm_start "$model_file" "$which_temp"
}

llamacpp_glm_q6_coding_start(){
    model_file="$1"
    if [[ -z $model_file ]]; then
        model_file="GLM-4.7-Flash-UD-Q6_K_XL.gguf"
    fi

    which_temp="$2"
    if [[ -z $model_file ]]; then
        which_temp="0.25"
    fi

    llamacpp_glm_start "$model_file" "$which_temp"
}


llamacpp_glm_q5_coding_start(){
    model_file="$1"
    if [[ -z $model_file ]]; then
        model_file="GLM-4.7-Flash-UD-Q5_K_XL.gguf"
    fi

    which_temp="$2"
    if [[ -z $model_file ]]; then
        which_temp="0.25"
    fi

    llamacpp_glm_start "$model_file" "$which_temp"
}


claude_glm_start() {
    reset_claude_environment_variables .

    export ANTHROPIC_AUTH_TOKEN="llamacpp"
    export ANTHROPIC_BASE_URL="http://localhost:8001"
    export CLAUDE_CODE_MAX_OUTPUT_TOKENS="64000"

    model="$1"
    if [[ -z $model ]]; then 
        model="unsloth/GLM-4.7-Flash"
    fi

    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        rm -rf ~/.claude/settings.json
        claude --model "$model"
    fi
}

claude_glm_ralph() {
    reset_claude_environment_variables .

    export ANTHROPIC_AUTH_TOKEN="llamacpp"
    export ANTHROPIC_BASE_URL="http://localhost:8001"
    export CLAUDE_CODE_MAX_OUTPUT_TOKENS="64000"
    # context size =   202752
    # output tokens =  64000
    # recomended was = 16384

    prompt_type="$1"
    shift
    prompt_data="$*"

    model="unsloth/GLM-4.7-Flash"

    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        rm -rf ~/.claude/settings.json
        if [[ $prompt_type == "file" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            cat $prompt_data | claude --dangerously-skip-permissions --model=$model
        elif [[ $prompt_type == "message" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            echo "$prompt_data" | claude --dangerously-skip-permissions --model=$model
        else
            echo "Unknown prompt_type: '$prompt_type', only supports: ('message', 'file')."
        fi
    fi
}


sglang_unsloth_glm_start(){
    model_file="$1"
    if [[ -z $model_file ]]; then
        model_file="GLM-4.7-Flash-UD-Q4_K_XL.gguf"
    fi

    # --model-path "$MODEL_LOCATIONS/unsloth/GLM-4.7-Flash/${model_file}" \

    python3 -m sglang.launch_server \
        --model-path "zai-org/GLM-4.7-Flash"
        --tp-size 4 \
        --tool-call-parser glm47  \
        --reasoning-parser glm45 \
        --speculative-algorithm EAGLE \
        --speculative-num-steps 3 \
        --speculative-eagle-topk 1 \
        --speculative-num-draft-tokens 4 \
        --mem-fraction-static 0.8 \
        --served-model-name glm-4.7-flash \
        --host 0.0.0.0 \
        --port 8000
}


glm_start(){
    python3 -m sglang.launch_server \
        --model-path "$MODEL_LOCATIONS/zai-org/GLM-4.7-Flash" \
        --tp-size 4 \
        --tool-call-parser glm47  \
        --reasoning-parser glm45 \
        --speculative-algorithm EAGLE \
        --speculative-num-steps 3 \
        --speculative-eagle-topk 1 \
        --speculative-num-draft-tokens 4 \
        --mem-fraction-static 0.8 \
        --served-model-name glm-4.7-flash \
        --host 0.0.0.0 \
        --port 8000
}

litellm_start(){
    if has_command "pip"; then
        if no_command "litellm"; then
            echo "Installing litellm"
            pip install 'litellm[proxy]'
        fi
    else
        echo "Unable to install litellm with pip, please setup"
    fi

    if no_command "litellm"; then
        echo "No litellm for me to run with"
    else
        litellm --config $DOTFILES/config/litellm/copilot-config.yaml
    fi
}


ollama_start(){
    if has_command "brew"; then
        if no_command "ollama"; then
            echo "Installing ollama"
            sudo pacman -S --noconfirm ollama-cuda cuda nvtop 
        fi
    else
        echo "Unable to install ollama with brew, please setup"
    fi

    if no_command "ollama"; then
        echo "No ollama for me to run with"
    else
        # # First, create the array
        # mapfile -t GPU_UUIDS < <(nvidia-smi -L | grep -oP '(?<=UUID: )GPU-[a-f0-9-]+')
        #
        # # Method 2: Join with comma
        # IFS=',' GPU_UUIDS_COMMA="${GPU_UUIDS[*]}"
        # echo "Identified GPU UUDs: $GPU_UUIDS_COMMA"
        #
        # export CUDA_VISIBLE_DEVICES="$GPU_UUIDS_COMMA"
        # echo "Setting CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"

        ollama serve
    fi
}


claude_ollama_start() {
    reset_claude_environment_variables .

    export ANTHROPIC_AUTH_TOKEN="ollama"
    export ANTHROPIC_BASE_URL="http://localhost:11434"

    model="$1"
    if [[ -z $model ]]; then 
        model="gpt-oss:20b"
        # model="qwen3-coder:30b"
    fi

    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        rm -rf ~/.claude/settings.json
        claude --model "$model"
    fi
}


test_llama_claude() {
    curl --location 'http://localhost:11434/v1/chat/completions' \
        --header "Authorization: Bearer ollama" \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "gpt-oss:20b",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
}

claude_ralph() {
    reset_claude_environment_variables

    export ANTHROPIC_AUTH_TOKEN="$LITELLM_MASTER_KEY"
    export ANTHROPIC_BASE_URL="http://localhost:4000"
    export ANTHROPIC_MODEL="claude-sonnet-4.5"
    export ANTHROPIC_SMALL_FAST_MODEL="gpt-4"
    export CLAUDE_CODE_MAX_OUTPUT_TOKENS="64000"

    prompt_type="$1"
    shift
    prompt_data="$*"

    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        rm -rf ~/.claude/settings.json
        if [[ $prompt_type == "file" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            cat $prompt_data | claude --dangerously-skip-permissions
        elif [[ $prompt_type == "message" ]]; then 
            # while :; do cat $prompt_file | claude --dangerously-skip-permissions; done
            echo "$prompt_data" | claude --dangerously-skip-permissions
        else
            echo "Unknown prompt_type: '$prompt_type', only supports: ('message', 'file')."
        fi
    fi
}

claude_start() {
    export ANTHROPIC_AUTH_TOKEN="$LITELLM_MASTER_KEY"
    export ANTHROPIC_BASE_URL="http://localhost:4000"
    export ANTHROPIC_MODEL="claude-sonnet-4.5"
    export ANTHROPIC_SMALL_FAST_MODEL="gpt-4"
    export CLAUDE_CODE_MAX_OUTPUT_TOKENS="64000"
    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        rm -rf ~/.claude/settings.json
        claude
    fi
}

test_gemini_flash() {
    curl --location 'http://0.0.0.0:4000/chat/completions' \
        --header "Authorization: Bearer $LITELLM_MASTER_KEY" \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "gemini-3-flash",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
}

test_claude_opus() {
    curl --location 'http://0.0.0.0:4000/chat/completions' \
        --header "Authorization: Bearer $LITELLM_MASTER_KEY" \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "claude-opus-4",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
}

test_claude_haiku() {
    curl --location 'http://0.0.0.0:4000/chat/completions' \
        --header "Authorization: Bearer $LITELLM_MASTER_KEY" \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "claude-haiku-4.5",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
}

test_claude_sonnet() {
    curl --location 'http://0.0.0.0:4000/chat/completions' \
        --header "Authorization: Bearer $LITELLM_MASTER_KEY" \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "claude-sonnet-4",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
    
}

test_claude_sonnet45() {
    curl --location 'http://0.0.0.0:4000/chat/completions' \
        --header "Authorization: Bearer $LITELLM_MASTER_KEY" \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "claude-sonnet-4.5",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
}

test_gpt4() {
    curl --location 'http://0.0.0.0:4000/chat/completions' \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "gpt-4",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
}

test_models() {
    curl --location 'http://0.0.0.0:4000/v1/models' \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' 
}
