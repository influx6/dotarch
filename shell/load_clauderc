#!/bin/bash

LITELLM_MASTER_KEY=litellm-647e8dd5-7197-4639-899f-463408b3fb44
LITELLM_SALT_KEY=litellm-3efab3d2-e033-46fe-976f-f2f05115ef98
ENABLE_NETWORK_MONITOR=true
LOG_LEVEL=DEBUG

# if has_command "claude"; then
#     curl -fsSL https://claude.ai/install.sh | bash
# fi

litellm_start(){
    if has_command "pip"; then
        if no_command "litellm"; then
            echo "Installing litellm"
            pip install 'litellm[proxy]'
        fi
    else
        echo "Unable to install litellm with pip, please setup"
    fi

    if no_command "litellm"; then
        echo "No litellm for me to run with"
    else
        litellm --config $DOTFILES/config/litellm/copilot-config.yaml
    fi
}

ollama_start(){
    if has_command "brew"; then
        if no_command "ollama"; then
            echo "Installing ollama"
            sudo pacman -S --noconfirm ollama-cuda cuda nvtop 
        fi
    else
        echo "Unable to install ollama with brew, please setup"
    fi

    if no_command "ollama"; then
        echo "No ollama for me to run with"
    else
        # # First, create the array
        # mapfile -t GPU_UUIDS < <(nvidia-smi -L | grep -oP '(?<=UUID: )GPU-[a-f0-9-]+')
        #
        # # Method 2: Join with comma
        # IFS=',' GPU_UUIDS_COMMA="${GPU_UUIDS[*]}"
        # echo "Identified GPU UUDs: $GPU_UUIDS_COMMA"
        #
        # export CUDA_VISIBLE_DEVICES="$GPU_UUIDS_COMMA"
        # echo "Setting CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"

        ollama serve
    fi
}


claude_llama_start() {
    unset ANTHROPIC_AUTH_TOKEN
    unset ANTHROPIC_BASE_URL
    unset ANTHROPIC_MODEL
    unset ANTHROPIC_SMALL_FAST_MODEL
    unset CLAUDE_CODE_MAX_OUTPUT_TOKENS

    export ANTHROPIC_AUTH_TOKEN="ollama"
    export ANTHROPIC_BASE_URL="http://localhost:11434"

    model="$1"
    if [[ -z $model ]]; then 
        # model="gpt-oss:20b"
        model="qwen3-coder:30b"
    fi

    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        claude --model "$model"
    fi
}

test_llama_claude() {
    curl --location 'http://localhost:11434/v1/chat/completions' \
        --header "Authorization: Bearer ollama" \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "gpt-oss:20b",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
}

claude_start() {
    export ANTHROPIC_AUTH_TOKEN="$LITELLM_MASTER_KEY"
    export ANTHROPIC_BASE_URL="http://localhost:4000"
    export ANTHROPIC_MODEL="claude-sonnet-4.5"
    export ANTHROPIC_SMALL_FAST_MODEL="gpt-4"
    export CLAUDE_CODE_MAX_OUTPUT_TOKENS="64000"
    if no_command "claude"; then
        if has_command "npm"; then
            npm install -g @anthropic-ai/claude-code
        else
            echo "Unable to install claude via npm, please setup"
        fi
    else
        claude
    fi
}

test_gemini_flash() {
    curl --location 'http://0.0.0.0:4000/chat/completions' \
        --header "Authorization: Bearer $LITELLM_MASTER_KEY" \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "gemini-3-flash",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
}

test_claude_opus() {
    curl --location 'http://0.0.0.0:4000/chat/completions' \
        --header "Authorization: Bearer $LITELLM_MASTER_KEY" \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "claude-opus-4",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
}

test_claude_haiku() {
    curl --location 'http://0.0.0.0:4000/chat/completions' \
        --header "Authorization: Bearer $LITELLM_MASTER_KEY" \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "claude-haiku-4.5",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
}

test_claude_sonnet() {
    curl --location 'http://0.0.0.0:4000/chat/completions' \
        --header "Authorization: Bearer $LITELLM_MASTER_KEY" \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "claude-sonnet-4",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
    
}

test_claude_sonnet45() {
    curl --location 'http://0.0.0.0:4000/chat/completions' \
        --header "Authorization: Bearer $LITELLM_MASTER_KEY" \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "claude-sonnet-4.5",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
}

test_gpt4() {
    curl --location 'http://0.0.0.0:4000/chat/completions' \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' \
        --data '{
        "model": "gpt-4",
        "messages": [
            {
            "role": "user",
            "content": "what llm are you"
            }
        ]
        }'
}

test_models() {
    curl --location 'http://0.0.0.0:4000/v1/models' \
        --header 'Content-Type: application/json' \
        --header 'Editor-Version: CommandLine/1.0' 
}
